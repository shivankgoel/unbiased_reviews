{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Data Loaded\n",
      "Businesses Data Loaded\n",
      "Review Data Loaded\n"
     ]
    }
   ],
   "source": [
    "from dependencies import *\n",
    "from helper import *\n",
    "from loaddata import review_data,business_data, rest_data, user_data, elite_udata, non_elite_udata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary:  15374\n"
     ]
    }
   ],
   "source": [
    "cv, bid_2_stars_ne, review_data_ne, bid_2_textenc = give_text_feats_tfidf(review_data)\n",
    "bid_2_stars_e = review_data.loc[review_data['iselite'] == True].groupby('business_id')['stars'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of Our Dataset:  (4771, 7)\n"
     ]
    }
   ],
   "source": [
    "xfeat,yfeat= create_dataset_novec(40,rest_data,bid_2_stars_ne,bid_2_stars_e,bid_2_textenc,False)\n",
    "xtrain, xtest, ytrain, ytest = give_train_test(xfeat, yfeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_novec(min_thold, bdata, bid_2_stars, bid_2_stars_e, bid_2_textenc, textenc = False, max_thold=float('inf')):\n",
    "    basicprint = False\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    y = []\n",
    "    bids = bdata['business_id'].values\n",
    "    for bid in bids:\n",
    "        if bid in bid_2_stars and bid in bid_2_stars_e and bid in bid_2_textenc:\n",
    "            stars_e = bid_2_stars_e[bid]\n",
    "            stars_ne = bid_2_stars[bid]\n",
    "            lestars = len(stars_e)\n",
    "            if(lestars >= min_thold and lestars < max_thold):\n",
    "                x = []\n",
    "                x.append([Counter(stars_ne)[i] for i in range(1,6)])\n",
    "                x.append([len(stars_ne)])\n",
    "                x.append([np.mean(stars_ne)])\n",
    "                if textenc:\n",
    "                    if basicprint:\n",
    "                        print('Num features before text: ', len([item for sublist in x for item in sublist]))\n",
    "                    x.append(bid_2_textenc[bid])\n",
    "                    if basicprint:\n",
    "                        print('Num features after text: ', len([item for sublist in x for item in sublist]))\n",
    "                        basicprint = False\n",
    "                x = [item for sublist in x for item in sublist]\n",
    "                x1.append(x)\n",
    "                y.append(np.mean(stars_e))                        \n",
    "    xfeat = np.array(x1)\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    xfeat = imp.fit_transform(xfeat)\n",
    "    yfeat = np.array(y).reshape(-1,)\n",
    "    print(\"Dimension of Our Dataset: \", xfeat.shape)\n",
    "    return xfeat,yfeat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear\n",
      "Margin  0.10  0.20  0.30  0.40  0.50\n",
      "        39.69  71.94  86.18  94.55  98.22\n",
      " \n",
      "Random Forest\n",
      "Margin  0.10  0.20  0.30  0.40  0.50\n",
      "        40.63  69.74  85.65  94.55  97.28\n",
      " \n",
      "Gradient Boost\n",
      "Margin  0.10  0.20  0.30  0.40  0.50\n",
      "        41.26  71.62  87.43  95.18  97.91\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, n_iter_no_change=None, presort='auto',\n",
       "             random_state=4, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = give_reg_model_linear()\n",
    "test_model(model,xtrain,ytrain,xtest,ytest)\n",
    "model = give_reg_model_rf(pcadim=7)\n",
    "test_model(model,xtrain,ytrain,xtest,ytest)\n",
    "model = give_reg_model_gb()\n",
    "test_model(model,xtrain,ytrain,xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
