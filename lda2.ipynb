{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Data Loaded\n",
      "Businesses Data Loaded\n",
      "Review Data Loaded\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from datasetcreator import *\n",
    "from dependencies import *\n",
    "from loaddata import review_data,business_data, rest_data, user_data, elite_udata, non_elite_udata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_text_feats_cv_elite(review_data):\n",
    "    review_data_ne = review_data.loc[review_data['iselite'] == True]\n",
    "    bid_2_stars = review_data_ne.groupby('business_id')['stars'].apply(list).to_dict()\n",
    "    bid_2_textrev = review_data_ne.groupby('business_id')['text'].apply(list).to_dict()\n",
    "    bid_2_text = {i:' '.join(j) for i,j in bid_2_textrev.items() if all(isinstance(x, str) for x in j)}\n",
    "    cv = CountVectorizer(min_df = 400, stop_words=\"english\")\n",
    "    textfeats = cv.fit_transform(bid_2_text.values())\n",
    "    print(\"Size of Vocabulary: \", len(cv.vocabulary_))\n",
    "    bid_2_textenc = {a:b.toarray()[0] for a,b in zip(bid_2_text.keys(),textfeats)}\n",
    "    return cv, textfeats, bid_2_stars, review_data_ne, bid_2_textenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary:  9763\n"
     ]
    }
   ],
   "source": [
    "cv, textfeats, bid_2_stars_ne, review_data_ne, bid_2_textenc = give_text_feats_cv_elite(review_data)\n",
    "bid_2_stars_e = review_data.loc[review_data['iselite'] == True].groupby('business_id')['stars'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plot_10_most_common_words(textfeats,cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "place good like just great time food really service don\n",
      "\n",
      "Topic #1:\n",
      "good food place like just really service great ordered restaurant\n"
     ]
    }
   ],
   "source": [
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below (use int values below 15)\n",
    "number_topics = 2\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(textfeats)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, cv, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "place good like just great time food really service don\n",
      "\n",
      "Topic #1:\n",
      "good food place like just really service great ordered restaurant\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda, cv, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
